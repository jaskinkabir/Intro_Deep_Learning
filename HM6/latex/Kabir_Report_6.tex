\documentclass{article}
%\usepackage[margin=1in]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{titling}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{minted}
\usepackage{url}
\usepackage{tabularx}
\usepackage{graphicx}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

\begin{document}
\input{title.tex}
\raggedright

\section{Problem 1: ViT From Scratch}
\subsection{Model Architectures}
This section focuses on training four
variations of a vision transformer based
network for image classification on the
CIFAR-100 Dataset. The four models shared the
same parameters other than the patch size and
number of attention layers.

The shared parameters are as follows:
\begin{itemize}
    \item Embedding Size: 192
    \item MLP Hidden Size: 384
    \item Number of Attention Heads: 4
\end{itemize}
The four variations are combinations of a
patch size of 4 or 8 and either 4 or 8
attention layers. The models were trained on
the CIFAR-100 dataset with a batch size of
1024, and learning rate of 5e-4. They were
also compared to a baseline ResNet-18
pretrained on ImageNet. 

\begin{table}[h]
    \centering % Center the table
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Parameters} & \textbf{MACs} & \textbf{Avg Epoch Time} & \textbf{Accuracy}  \\
        \hline
        \textbf{ViT Patch4 Attn4} & 1,377,508 & 756,480 & 11.76s & 43.9\% \\
        \hline
        \textbf{ViT Patch4 Attn8} & 2,565,604 & 756,480 & 15.07s & 44.6\% \\
        \hline
        \textbf{ViT Patch8 Attn4} & 1,395,940 & 756,480 & 10.58s & 37.9\% \\
        \hline
        \textbf{ViT Patch8 Attn8} & 2,584,036 & 756,480 & 11.53s & 37.7\% \\
        \hline
        \textbf{ResNet-18} & 11,227,812 & 1,816,096,768 & 30.07s & 56.9\% \\
    \end{tabular}
    \caption{ViT Model Comparisons}
    \label{tab:complexities}
\end{table}


\end{document}