{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jlib.transformer_char_predictor import TransformerCharPredictor\n",
    "import jlib.data_utils as data_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchprofile import profile_macs\n",
    "text = \"\"\n",
    "with open('data/sequence.txt', 'r') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeqLen 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 10\n",
    "def train_and_plot(seqlen: int):\n",
    "    data = data_utils.gen_datasets(text, seqlen)\n",
    "    train_data = data['train_dataset']\n",
    "    val_data = data['val_dataset']\n",
    "    alphabet: data_utils.Alphabet = data['alphabet']\n",
    "\n",
    "    train_fetcher = data_utils.gen_data_loader(\n",
    "        train_data,\n",
    "        batch_size=32,\n",
    "        workers = 6,\n",
    "        cpu_prefetch= 20,\n",
    "        gpu_prefetch=20\n",
    "    )\n",
    "\n",
    "    val_fetcher = data_utils.gen_data_loader(\n",
    "        val_data,\n",
    "        batch_size=len(val_data),\n",
    "        workers = 6,\n",
    "        cpu_prefetch= 10,\n",
    "        gpu_prefetch=10\n",
    "    )\n",
    "\n",
    "    # model\n",
    "\n",
    "    model = TransformerCharPredictor(\n",
    "        alphabet_size = len(alphabet),\n",
    "        max_len = seqlen,\n",
    "        hidden_dim = 128,\n",
    "        inner_dim = 2048,\n",
    "        num_attn_heads = 2,\n",
    "        num_attn_layers=3,\n",
    "        cls_head_dims=[],\n",
    "        dropout = 0.1\n",
    "    )\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameter count: {param_count:,}\")\n",
    "\n",
    "    test_input = next(iter(train_fetcher))[0]\n",
    "\n",
    "    print(f\"Model MACs: {profile_macs(model, test_input):,}\")\n",
    "    \n",
    "# Model parameter count: 1,790,380\n",
    "# Model MACs: 568,279,052\n",
    "\n",
    "\n",
    "\n",
    "    model.train_model(\n",
    "        epochs=100,\n",
    "        train_fetcher=train_fetcher,\n",
    "        val_fetcher=val_fetcher,\n",
    "        optimizer = torch.optim.Adam,\n",
    "        optimizer_kwargs={\n",
    "            'lr': 3e-3,\n",
    "            'betas': (0.9, 0.98),\n",
    "            'eps': 1e-9,\n",
    "            'weight_decay': 1e-5\n",
    "        },\n",
    "        min_accuracy=1,\n",
    "        max_negative_diff_count=10,\n",
    "        save_path=f'models/p1-{seqlen}.pth',\n",
    "        stop_on_plateau=True,\n",
    "    )\n",
    "\n",
    "    fig = model.plot_training(f'Small Corpus, Sequence Length {seqlen}')\n",
    "    fig.savefig(f'latex/images/p1-{seqlen}.png')\n",
    "    \n",
    "    del train_fetcher, val_fetcher, train_data, val_data, data, model, alphabet\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin init data loader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 0.00244140625 MiB\n",
      "Data Loader init time: 0.803153 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 0.877475 s\n",
      "Begin init data loader\n",
      "Batch Size: 0.03631591796875 MiB\n",
      "Data Loader init time: 0.104730 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 0.169078 s\n",
      "Model parameter count: 1,790,380\n",
      "Model MACs: 568,279,052\n",
      "Training TransformerCharPredictor\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unflatten\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scaled_dot_product_attention\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |  Validation Loss   |Validation Accuracy |   Î” Accuracy (%)   |    Memory Usage    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      1.757319      |      2.874692      |      2.211866      |     44.222689      |      0.000000      |      0.163653      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      1.426046      |      1.906531      |      1.048573      |     78.571427      |     77.672211      |      0.167559      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      1.500337      |      1.243135      |      0.703344      |     84.726888      |      7.834224      |      0.171466      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      1.520533      |      1.044345      |      0.601600      |     85.672271      |      1.115801      |      0.175372      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      1.320205      |      0.983530      |      0.573376      |     85.441178      |     -0.269741      |      0.179278      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      1.280262      |      0.978335      |      0.570735      |     85.378152      |     -0.343307      |      0.183185      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      1.408762      |      0.936055      |      0.548526      |     85.945380      |      0.318783      |      0.187091      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      1.389028      |      0.914152      |      0.585177      |     85.357141      |     -0.684433      |      0.190997      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      1.318150      |      0.918502      |      0.556877      |     85.462183      |     -0.562214      |      0.194903      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      1.422372      |      0.928268      |      0.534904      |     86.218488      |      0.317770      |      0.198810      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      1.428289      |      0.901807      |      0.516206      |     86.638653      |      0.487326      |      0.202716      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      1.428769      |      0.903928      |      0.524936      |     86.575627      |     -0.072746      |      0.206622      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      1.425158      |      0.888895      |      0.548515      |     85.924369      |     -0.824441      |      0.210528      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      1.430916      |      0.895962      |      0.526564      |     86.302519      |     -0.387973      |      0.214435      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      1.513901      |      0.893580      |      0.512399      |     86.470586      |     -0.193986      |      0.218341      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[0;34m(seqlen)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel MACs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile_macs(model,\u001b[38;5;250m \u001b[39mtest_input)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Model parameter count: 1,790,380\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Model MACs: 568,279,052\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_fetcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_fetcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_fetcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_fetcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_negative_diff_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/p1-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseqlen\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_on_plateau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     fig \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmall Corpus, Sequence Length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseqlen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m     fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatex/images/p1-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseqlen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:234\u001b[0m, in \u001b[0;36mTransformerCharPredictor.train_model\u001b[0;34m(self, epochs, train_fetcher, val_fetcher, stop_on_plateau, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch, sched_factor, sched_patience, min_accuracy, max_negative_diff_count, save_path)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    231\u001b[0m     begin_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()             \n\u001b[0;32m--> 234\u001b[0m     epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     epoch_val_loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_step(val_fetcher)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_hist[epoch] \u001b[38;5;241m=\u001b[39m epoch_train_loss\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:145\u001b[0m, in \u001b[0;36mTransformerCharPredictor.train_step\u001b[0;34m(self, fetcher)\u001b[0m\n\u001b[1;32m    143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m    144\u001b[0m     train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), Y_batch)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()                   \n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_plot(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
