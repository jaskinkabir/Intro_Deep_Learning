{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jlib.transformer_char_predictor import TransformerCharPredictor\n",
    "import jlib.data_utils as data_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchprofile import profile_macs\n",
    "text = \"\"\n",
    "with open('data/sequence.txt', 'r') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeqLen 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 10\n",
    "def train_and_plot(seqlen: int):\n",
    "    data = data_utils.gen_datasets(text, seqlen)\n",
    "    train_data = data['train_dataset']\n",
    "    val_data = data['val_dataset']\n",
    "    alphabet: data_utils.Alphabet = data['alphabet']\n",
    "\n",
    "    train_fetcher = data_utils.gen_data_loader(\n",
    "        train_data,\n",
    "        batch_size=32,\n",
    "        workers = 6,\n",
    "        cpu_prefetch= 20,\n",
    "        gpu_prefetch=20\n",
    "    )\n",
    "\n",
    "    val_fetcher = data_utils.gen_data_loader(\n",
    "        val_data,\n",
    "        batch_size=len(val_data),\n",
    "        workers = 6,\n",
    "        cpu_prefetch= 10,\n",
    "        gpu_prefetch=10\n",
    "    )\n",
    "\n",
    "    # model\n",
    "\n",
    "    model = TransformerCharPredictor(\n",
    "        alphabet_size = len(alphabet),\n",
    "        max_len = seqlen,\n",
    "        hidden_dim = 1024,\n",
    "        inner_dim = 4096,\n",
    "        num_attn_heads = 8,\n",
    "        num_attn_layers=6,\n",
    "        cls_head_dims=[1024, 512],\n",
    "        dropout = 0.1\n",
    "    )\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameter count: {param_count:,}\")\n",
    "\n",
    "    # test_input = next(iter(train_fetcher))[0]\n",
    "\n",
    "    # print(f\"Model MACs: {profile_macs(model, test_input):,}\")\n",
    "    \n",
    "#Model parameter count: 77,219,372\n",
    "#Model MACs: 49,602,887,704\n",
    "\n",
    "\n",
    "\n",
    "    model.train_model(\n",
    "        epochs=100,\n",
    "        train_fetcher=train_fetcher,\n",
    "        val_fetcher=val_fetcher,\n",
    "        optimizer = torch.optim.Adam,\n",
    "        optimizer_kwargs={\n",
    "            'lr': 3e-4,\n",
    "            'betas': (0.9, 0.98),\n",
    "            'eps': 1e-9,\n",
    "            'weight_decay': 1e-5\n",
    "        },\n",
    "        min_accuracy=1,\n",
    "        max_negative_diff_count=10,\n",
    "        save_path=f'models/p1-{seqlen}.pth',\n",
    "        stop_on_plateau=True\n",
    "    )\n",
    "\n",
    "    model.plot_training(f'Small Corpus, Sequece Length {seqlen}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin init data loader\n",
      "Batch Size: 0.0048828125 MiB\n",
      "Data Loader init time: 0.122642 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 0.216636 s\n",
      "Begin init data loader\n",
      "Batch Size: 0.07232666015625 MiB\n",
      "Data Loader init time: 0.125589 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 0.213814 s\n",
      "Model parameter count: 77,219,372\n",
      "Training TransformerCharPredictor\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |  Validation Loss   |Validation Accuracy |   Δ Accuracy (%)   |    Memory Usage    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      3.212504      |      2.928128      |      2.579155      |     27.911392      |      0.000000      |      5.393523      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      3.261614      |      2.745400      |      2.518794      |     28.238398      |      1.171586      |      5.397429      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      3.452933      |      2.707124      |      2.533492      |     27.805907      |     -1.531572      |      5.401336      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      3.056693      |      2.698432      |      2.525431      |     28.428271      |      0.672395      |      5.405242      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      3.373867      |      2.699174      |      2.519937      |     28.691983      |      0.927639      |      5.409149      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      3.472468      |      2.689061      |      2.505666      |     28.554854      |     -0.477936      |      5.413055      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      3.173120      |      2.686888      |      2.506521      |     27.911392      |     -2.720591      |      5.416961      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      3.145751      |      2.694063      |      2.518465      |     28.691983      |      0.000000      |      5.420867      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      3.026000      |      2.685308      |      2.510047      |     28.597045      |     -0.330888      |      5.424774      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      3.130634      |      2.688837      |      2.509823      |     27.721518      |     -3.382356      |      5.428680      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      3.206931      |      2.691059      |      2.501533      |     28.818566      |      0.441177      |      5.432586      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      3.303537      |      2.686942      |      2.513432      |     28.206751      |     -2.122988      |      5.436492      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      3.144670      |      2.675343      |      2.485788      |     28.945148      |      0.439239      |      5.438446      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      3.430620      |      2.682230      |      2.506066      |     28.691983      |     -0.874636      |      5.438446      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      3.167860      |      2.681814      |      2.487155      |     28.734177      |     -0.728864      |      5.438446      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |  Validation Loss   |Validation Accuracy |   Δ Accuracy (%)   |    Memory Usage    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         15         |      3.112337      |      2.664721      |      2.492567      |     28.765821      |     -0.619539      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         16         |      3.208417      |      2.673230      |      2.492063      |     29.187763      |      0.838188      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         17         |      3.568978      |      2.672236      |      2.488564      |     28.597045      |     -2.023854      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         18         |      3.155576      |      2.684659      |      2.485278      |     29.113925      |     -0.252977      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         19         |      3.135551      |      2.683327      |      2.498257      |     28.554854      |     -2.168405      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         20         |      3.093471      |      2.677499      |      2.493689      |     28.618142      |     -1.951574      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         21         |      3.118941      |      2.678731      |      2.491501      |     28.628692      |     -1.915428      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         22         |      3.137815      |      2.672121      |      2.487249      |     28.955695      |     -0.795086      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         23         |      3.096662      |      2.673582      |      2.484503      |     29.113925      |     -0.252977      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         24         |      3.150752      |      2.667986      |      2.479939      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         25         |      3.151289      |      2.665853      |      2.480121      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         26         |      3.138183      |      2.667588      |      2.478596      |     29.145569      |     -0.144561      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         27         |      3.162291      |      2.665545      |      2.478699      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         28         |      3.214496      |      2.658783      |      2.482315      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         29         |      3.143147      |      2.658450      |      2.480498      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |  Validation Loss   |Validation Accuracy |   Δ Accuracy (%)   |    Memory Usage    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         30         |      3.105152      |      2.661456      |      2.480936      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         31         |      3.115213      |      2.655193      |      2.480898      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         32         |      3.212921      |      2.663232      |      2.480831      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         33         |      3.161863      |      2.662898      |      2.480835      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         34         |      3.132493      |      2.667142      |      2.480998      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         35         |      3.138323      |      2.665056      |      2.480998      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         36         |      3.244842      |      2.655168      |      2.480978      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         37         |      3.197639      |      2.662644      |      2.481047      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         38         |      3.173277      |      2.665069      |      2.481000      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         39         |      3.135651      |      2.666643      |      2.481068      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         40         |      3.227683      |      2.655466      |      2.481047      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         41         |      3.230287      |      2.663419      |      2.481067      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         42         |      3.192835      |      2.670799      |      2.481084      |     29.156119      |     -0.108416      |      5.442352      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[0;34m(seqlen)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel parameter count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# test_input = next(iter(train_fetcher))[0]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(f\"Model MACs: {profile_macs(model, test_input):,}\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#Model parameter count: 77,219,372\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#Model MACs: 49,602,887,704\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_fetcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_fetcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_fetcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_fetcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_negative_diff_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/p1-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseqlen\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     model\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmall Corpus, Sequece Length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseqlen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:328\u001b[0m, in \u001b[0;36mTransformerCharPredictor.train_model\u001b[0;34m(self, epochs, train_fetcher, val_fetcher, stop_on_plateau, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch, sched_factor, sched_patience, min_accuracy, max_negative_diff_count, save_path)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    325\u001b[0m     begin_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()             \n\u001b[0;32m--> 328\u001b[0m     epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     epoch_val_loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_step(val_fetcher)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_hist[epoch] \u001b[38;5;241m=\u001b[39m epoch_train_loss\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:237\u001b[0m, in \u001b[0;36mTransformerCharPredictor.train_step\u001b[0;34m(self, fetcher)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 237\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), Y_batch)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(train_batch_loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:221\u001b[0m, in \u001b[0;36mTransformerCharPredictor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 221\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:171\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(sequences)\n\u001b[1;32m    169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[0;32m--> 171\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM5/jlib/transformer_char_predictor.py:129\u001b[0m, in \u001b[0;36m_TransformerEncoderLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Feed forward network\u001b[39;00m\n\u001b[1;32m    128\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n\u001b[0;32m--> 129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mff_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2902\u001b[0m         layer_norm,\n\u001b[1;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2909\u001b[0m     )\n\u001b[0;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_plot(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
