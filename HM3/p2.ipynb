{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from jlib.classifier import Classifier\n",
    "from jlib.get_shakespeare_loaders import get_shakespeare_loaders\n",
    "\n",
    "text = \"\"\n",
    "with open('data/sequence.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "class ShakespeareRNN(Classifier):\n",
    "    def __init__(self, alphabet_size, hidden_size, rnn=nn.RNN, linear_network=[]):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(alphabet_size, hidden_size)\n",
    "        self.rnn = rnn(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential()\n",
    "        linear_in = hidden_size\n",
    "        linear_out = alphabet_size\n",
    "        for i, layer_size in enumerate(linear_network):\n",
    "            self.fc.add_module(f'linear_{i}', nn.Linear(linear_in, layer_size))\n",
    "            self.fc.add_module(f'relu_{i}', nn.ReLU())\n",
    "            linear_in = layer_size\n",
    "        self.fc.add_module('final_linear', nn.Linear(linear_in, linear_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x[:, -1, :])  # Get the output of the last RNN cell\n",
    "        return x\n",
    "\n",
    "def train_and_plot(data, model: ShakespeareRNN, name, *training_args, **training_kwargs):\n",
    "    print(f\"Training {name}\")\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {param_count} parameters\")\n",
    "    model.train_model(\n",
    "        *training_args,\n",
    "        **training_kwargs,\n",
    "        train_loader=data['train_loader'],\n",
    "        val_loader=data['val_loader'],\n",
    "    )\n",
    "    fig = model.plot_training(f\"{name} Training\")\n",
    "    fig.savefig(f\"images/{name}_training_new.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train GPU Prefetch: 819200.0\n",
      "Train CPU Prefetch: 81920.0\n",
      "Val GPU Prefetch: 25600.0\n",
      "Val CPU Prefetch: 5120.0\n",
      "Train Loader\n",
      "Begin init data loader\n",
      "Batch Size: 0.0048828125 MiB\n",
      "Data Loader init time: 2.510459 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 3.874673 s\n",
      "Val Loader\n",
      "Begin init data loader\n",
      "Batch Size: 0.078125 MiB\n",
      "Data Loader init time: 1.123367 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 1.301090 s\n"
     ]
    }
   ],
   "source": [
    "data_20 = get_shakespeare_loaders(\n",
    "    train_batch_size=32,\n",
    "    val_batch_size=512,\n",
    "    redownload=False,\n",
    "    sequence_length=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM-20\n",
      "Model has 138778 parameters\n",
      "Training ShakespeareRNN\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "training\n",
      "Max: 63 Min: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [249,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m lstm20 \u001b[38;5;241m=\u001b[39m ShakespeareRNN(\n\u001b[1;32m      2\u001b[0m     alphabet_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m26\u001b[39m,\n\u001b[1;32m      3\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      4\u001b[0m     rnn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLSTM,\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_and_plot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_20\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlstm20\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM-20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_accuracy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[0;34m(data, model, name, *training_args, **training_kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m param_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m fig \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_training_new.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM3/jlib/classifier.py:157\u001b[0m, in \u001b[0;36mClassifier.train_model\u001b[0;34m(self, epochs, train_loader, val_loader, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch, sched_factor, sched_patience, min_accuracy, max_negative_diff_count)\u001b[0m\n\u001b[1;32m    155\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred, Y_batch) \n\u001b[1;32m    159\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mShakespeareRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 27\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Get the output of the last RNN cell\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "lstm20 = ShakespeareRNN(\n",
    "    alphabet_size=26,\n",
    "    hidden_size=128,\n",
    "    rnn=nn.LSTM,\n",
    ").to('cuda')\n",
    "train_and_plot(\n",
    "    data_20,\n",
    "    lstm20,\n",
    "    \"LSTM-20\",\n",
    "    epochs=100,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs = {'lr': 0.001},\n",
    "    min_accuracy = 0.7\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
