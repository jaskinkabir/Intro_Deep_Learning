{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaskin/Intro_Deep_Learning/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin init data loader\n",
      "Batch Size: 6.0 MiB\n",
      "Data Size: 292.96875 GiB\n",
      "Data Loader init time: 0.832833 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 0.959492 s\n",
      "Begin init data loader\n",
      "Batch Size: 24.0 MiB\n",
      "Data Size: 234.375 GiB\n",
      "Data Loader init time: 1.664529 s\n",
      "Begin init fetcher\n",
      "Fetcher init time: 1.710639 s\n",
      "Model: VggNet CIFAR100 DO\n",
      "--------------------------------------------------\n",
      "Num Params: 55876964\n",
      "Training VggNet\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Î” Accuracy (%)   |  Validation Time   |  GPU Memory (GiB)  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |     87.346412      |      4.574911      |      4.485846      |     -1.946800      |      1.130000      |      0.000000      |     35.353639      |      9.501313      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |     86.002661      |      4.385881      |      4.273371      |     -2.565269      |      3.090000      |     173.451327     |     32.580555      |      9.677094      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |     118.932126     |      4.217132      |      4.124353      |     -2.200031      |      5.980000      |     93.527508      |     48.927530      |      9.852875      |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m\n\u001b[1;32m     87\u001b[0m Vgg_100_dp \u001b[38;5;241m=\u001b[39m VggNet(\n\u001b[1;32m     88\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     89\u001b[0m     dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marchitecture\n\u001b[1;32m     91\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m cifar_100_train_loader, cifar_100_val_loader \u001b[38;5;241m=\u001b[39m get_cifar_loaders(\n\u001b[1;32m     94\u001b[0m     is_cifar10\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m     train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     val_gpu_prefetch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVgg_100_dp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar_100_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar_100_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVggNet CIFAR100 DO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m delete_deletables([Vgg_100_dp])\n\u001b[1;32m    111\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[0;34m(model, train, val, title, min_accuracy)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msched_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msched_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_accuracy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_negative_diff_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m fig \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/HM2/jlib/classifier.py:157\u001b[0m, in \u001b[0;36mClassifier.train_model\u001b[0;34m(self, epochs, train_loader, val_loader, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch, sched_factor, sched_patience, min_accuracy, max_negative_diff_count)\u001b[0m\n\u001b[1;32m    155\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred, Y_batch) \n\u001b[1;32m    156\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 157\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()                   \n\u001b[1;32m    160\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Intro_Deep_Learning/env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from helpers import *\n",
    "\n",
    "from jlib.cifar_preprocessing import get_cifar_loaders, delete_deletables\n",
    "from jlib.classifier import Classifier\n",
    "from jlib.vggnet import VggNet, VggBlock, ConvParams\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'\n",
    "\n",
    "\"\"\"\n",
    "sudo fuser -v -k /usr/lib/wsl/drivers/nvhm.inf_amd64_5c197d2d97068bef/*\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_plot(model: Classifier, train, val, title, min_accuracy=0.65):\n",
    "    print(f\"Model: {title}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Num Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "    model.train_model(\n",
    "        epochs=50,\n",
    "        train_loader=train,\n",
    "        val_loader=val,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer_args=[],\n",
    "        optimizer_kwargs={'lr': 1e-4 },\n",
    "        print_epoch=1,\n",
    "        header_epoch=10,\n",
    "        sched_factor=0.1,\n",
    "        sched_patience=5,\n",
    "        min_accuracy = 1,\n",
    "        max_negative_diff_count = 7\n",
    "    )\n",
    "    torch.save(model, f'models/{title}.pth')\n",
    "    fig = model.plot_training(f\"Training {title}\")\n",
    "    fig.savefig(f'figures/{title}.png')\n",
    "    plt.show()\n",
    "    #plt.savefig(f'figures/{title}.png')\n",
    "\n",
    "# Num Params original vgg-19 is 3.428967e+08\n",
    "architecture = {\n",
    "    'in_chan': 3,\n",
    "    'in_dim': (32, 32),\n",
    "    'block_params': [\n",
    "        VggBlock(\n",
    "            params=ConvParams(kernel=3,out_chan=256),\n",
    "            pool_kernel=2,\n",
    "            pool_stride=2,\n",
    "            repititions=2\n",
    "        ),\n",
    "        VggBlock(\n",
    "            params=ConvParams(kernel=3,out_chan=256),\n",
    "            pool_kernel=2,\n",
    "            pool_stride=1,\n",
    "            repititions=2\n",
    "        ),\n",
    "        VggBlock(\n",
    "            params=ConvParams(kernel=3,out_chan=512),\n",
    "            pool_kernel=2,\n",
    "            pool_stride=1,\n",
    "            repititions=4\n",
    "        ),\n",
    "        VggBlock(\n",
    "            params=ConvParams(kernel=3,out_chan=256),\n",
    "            pool_kernel=2,\n",
    "            pool_stride=1,\n",
    "            repititions=4\n",
    "        ),\n",
    "        VggBlock(\n",
    "            params=ConvParams(kernel=3,out_chan=128),\n",
    "            pool_kernel=2,\n",
    "            pool_stride=1,\n",
    "            repititions=4\n",
    "        ),\n",
    "    ],\n",
    "    'fc_params': [\n",
    "        2048,\n",
    "        2048,\n",
    "    ],\n",
    "}\n",
    "torch.cuda.empty_cache()\n",
    "Vgg_100_dp = VggNet(\n",
    "    num_classes=100,\n",
    "    dropout = 0.5,\n",
    "    **architecture\n",
    ").to(device)\n",
    "\n",
    "cifar_100_train_loader, cifar_100_val_loader = get_cifar_loaders(\n",
    "    is_cifar10=False,\n",
    "    train_batch_size=512,\n",
    "    val_batch_size=2048,\n",
    "    train_workers=6,\n",
    "    train_cpu_prefetch=10,\n",
    "    train_gpu_prefetch=10,\n",
    "    val_workers=2,\n",
    "    val_cpu_prefetch=2,\n",
    "    val_gpu_prefetch=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_and_plot(Vgg_100_dp, cifar_100_train_loader, cifar_100_val_loader, \"VggNet CIFAR100 DO\")\n",
    "delete_deletables([Vgg_100_dp])\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "Vgg_100_ndp = VggNet(\n",
    "    num_classes=100,\n",
    "    dropout = 0,\n",
    "    **architecture\n",
    ").to(device)\n",
    "\n",
    "train_and_plot(Vgg_100_ndp, cifar_100_train_loader, cifar_100_val_loader, \"VggNet CIFAR100 NO DO\")\n",
    "delete_deletables([Vgg_100_ndp])\n",
    "\n",
    "delete_deletables([cifar_100_train_loader, cifar_100_val_loader])\n",
    "\n",
    "cifar_10_train_loader, cifar_10_val_loader = get_cifar_loaders(\n",
    "    is_cifar10=True,\n",
    "    train_batch_size=512,\n",
    "    val_batch_size=512,\n",
    "    train_workers=6,\n",
    "    train_cpu_prefetch=10,\n",
    "    train_gpu_prefetch=10,\n",
    "    val_workers=2,\n",
    "    val_cpu_prefetch=2,\n",
    "    val_gpu_prefetch=2,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "Vgg_10_dp = VggNet(\n",
    "    num_classes=10,\n",
    "    dropout = 0.5,\n",
    "    **architecture\n",
    ").to(device)\n",
    "\n",
    "train_and_plot(Vgg_10_dp, cifar_10_train_loader, cifar_10_val_loader, \"VggNet CIFAR10 DO\")\n",
    "delete_deletables([Vgg_10_dp])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "Vgg_10_ndp = VggNet(\n",
    "    num_classes=10,\n",
    "    dropout = 0,\n",
    "    **architecture\n",
    ").to(device)\n",
    "\n",
    "train_and_plot(Vgg_10_ndp, cifar_10_train_loader, cifar_10_val_loader, \"VggNet CIFAR10 NO DO\")\n",
    "del Vgg_10_ndp\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
